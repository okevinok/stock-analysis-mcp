#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Reuters MCP Server

è¿™æ˜¯ä¸€ä¸ªåŸºäºReuters APIçš„æ–°é—»æ•°æ®MCPæœåŠ¡å™¨ï¼Œä½¿ç”¨FastMCPæ¡†æ¶æ„å»ºã€‚
å®ƒæä¾›äº†å¯¹Reutersæ–°é—»æ•°æ®çš„è®¿é—®ï¼Œé€šè¿‡MCPåè®®æš´éœ²Reuters APIã€‚
"""

import sys
import os
from typing import Dict, List, Optional, Any
from datetime import datetime
import pandas as pd

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥reuters_client
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from reuters_client import (
    ReutersClient, 
    ApiError, 
    RedirectError, 
    ExternalError, 
    InternalError,
    Article,
    Articles,
    Topic,
    Section
)

from fastmcp import FastMCP

# é™åˆ¶è¿”å›çš„æœ€å¤§æ•°æ®è¡Œæ•°
MAX_DATA_ROW = 50

# åˆ›å»ºMCPæœåŠ¡å™¨å®ä¾‹
mcp = FastMCP("Reutersæ–°é—»æ•°æ®æœåŠ¡", dependencies=["requests>=2.25.0"])

# åˆ›å»ºå…¨å±€Reuterså®¢æˆ·ç«¯å®ä¾‹
reuters_client = ReutersClient(timeout=30)

# é€šç”¨çš„æ•°æ®å¤„ç†å‡½æ•°
def process_articles_result(articles: List[Article], description: str = "articles") -> dict:
    """å¤„ç†æ–‡ç« åˆ—è¡¨ç»“æœï¼Œç¡®ä¿è¿”å›æ­£ç¡®çš„å­—å…¸æ ¼å¼"""
    try:
        # é™åˆ¶è¿”å›çš„æ–‡ç« æ•°é‡
        limited_articles = articles[:min(MAX_DATA_ROW, len(articles))]
        
        # å°†Articleå¯¹è±¡è½¬æ¢ä¸ºå­—å…¸
        articles_data = []
        for article in limited_articles:
            article_dict = {
                "title": article.title,
                "canonical_url": article.canonical_url,
                "description": article.description,
                "published_time": article.published_time,
                "subtype": article.subtype,
                "authors": [{"name": author.name, "topic_url": author.topic_url, "byline": author.byline} 
                           for author in article.authors] if article.authors else [],
                "thumbnail": {
                    "caption": article.thumbnail.caption,
                    "width": article.thumbnail.width,
                    "height": article.thumbnail.height,
                    "resizer_url": article.thumbnail.resizer_url
                } if article.thumbnail else None
            }
            articles_data.append(article_dict)
        
        return {
            "success": True,
            "count": len(limited_articles),
            "total_count": len(articles),
            description: articles_data
        }
    except Exception as e:
        return {
            "success": False,
            "error": str(e),
            description: []
        }

def process_articles_with_pagination_result(articles_result: Articles, description: str = "articles") -> dict:
    """å¤„ç†å¸¦åˆ†é¡µçš„æ–‡ç« ç»“æœ"""
    try:
        articles_data = []
        
        if articles_result.articles:
            # é™åˆ¶è¿”å›çš„æ–‡ç« æ•°é‡
            limited_articles = articles_result.articles[:min(MAX_DATA_ROW, len(articles_result.articles))]
            
            # å°†Articleå¯¹è±¡è½¬æ¢ä¸ºå­—å…¸
            for article in limited_articles:
                article_dict = {
                    "title": article.title,
                    "canonical_url": article.canonical_url,
                    "description": article.description,
                    "published_time": article.published_time,
                    "subtype": article.subtype,
                    "authors": [{"name": author.name, "topic_url": author.topic_url, "byline": author.byline} 
                               for author in article.authors] if article.authors else [],
                    "thumbnail": {
                        "caption": article.thumbnail.caption,
                        "width": article.thumbnail.width,
                        "height": article.thumbnail.height,
                        "resizer_url": article.thumbnail.resizer_url
                    } if article.thumbnail else None
                }
                articles_data.append(article_dict)
        
        # å¤„ç†ä¸»é¢˜ä¿¡æ¯
        topics_data = []
        if articles_result.topics:
            for topic in articles_result.topics:
                topic_dict = {
                    "name": topic.name,
                    "topic_url": topic.topic_url,
                    "byline": topic.byline
                }
                topics_data.append(topic_dict)
        
        return {
            "success": True,
            "count": len(articles_data),
            "total_count": articles_result.pagination.total_size,
            "pagination": {
                "total_size": articles_result.pagination.total_size
            },
            description: articles_data,
            "topics": topics_data
        }
    except Exception as e:
        return {
            "success": False,
            "error": str(e),
            description: [],
            "topics": []
        }

# å·¥å…·å‡½æ•°ï¼šè·å–å½“å‰æ—¶é—´
@mcp.tool()
def mcp_get_current_time() -> dict:
    """è·å–å½“å‰æ—¶é—´
    
    è·å–å½“å‰æ—¶é—´æ•°æ®
    
    Returns:
        dict: åŒ…å«å½“å‰æ—¶é—´çš„å­—å…¸
    """
    current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    return {"current_time": current_time}

# å·¥å…·å‡½æ•°ï¼šæœç´¢Reutersæ–‡ç« 
@mcp.tool()
def mcp_search_articles(keyword: str, offset: int = 0, size: int = 20) -> dict:
    """æŒ‰å…³é”®è¯æœç´¢Reutersæ–‡ç« 
    
    æ•°æ®æ¥æº: Reuters API - æ–‡ç« æœç´¢
    
    Args:
        keyword: æœç´¢å…³é”®è¯ï¼Œå¦‚"technology"ã€"artificial intelligence"ç­‰
        offset: åç§»é‡ï¼Œç”¨äºåˆ†é¡µï¼Œé»˜è®¤0
        size: è¿”å›æ–‡ç« æ•°é‡ï¼Œé»˜è®¤20ï¼Œæœ€å¤§50
        
    Returns:
        dict: åŒ…å«æœç´¢ç»“æœçš„å­—å…¸ï¼ŒåŒ…æ‹¬æ–‡ç« åˆ—è¡¨ã€åˆ†é¡µä¿¡æ¯ç­‰
    """
    try:
        # é™åˆ¶sizeçš„æœ€å¤§å€¼
        size = min(size, MAX_DATA_ROW)
        
        search_results = reuters_client.search_articles(keyword=keyword, offset=offset, size=size)
        return process_articles_with_pagination_result(search_results, "search_results")
    except ApiError as e:
        return {
            "success": False,
            "error": f"Reuters APIé”™è¯¯: {str(e)}",
            "search_results": []
        }
    except Exception as e:
        return {
            "success": False,
            "error": f"æœç´¢å¤±è´¥: {str(e)}",
            "search_results": []
        }

# å·¥å…·å‡½æ•°ï¼šé€šè¿‡URLè·å–æ–‡ç« è¯¦æƒ…
@mcp.tool()
def mcp_article_by_url(article_path: str) -> dict:
    """é€šè¿‡URLè·¯å¾„è·å–æ–‡ç« è¯¦æƒ…
    
    æ•°æ®æ¥æº: Reuters API - æ–‡ç« è¯¦æƒ…
    
    Args:
        article_path: æ–‡ç« URLè·¯å¾„ï¼Œå¦‚"/business/energy/oil-prices-rise-2024-01-15/"
        
    Returns:
        dict: åŒ…å«æ–‡ç« è¯¦æƒ…çš„å­—å…¸ï¼ŒåŒ…æ‹¬å®Œæ•´å†…å®¹ã€ä½œè€…ä¿¡æ¯ç­‰
    """
    try:
        article = reuters_client.fetch_article_by_url(article_path)
        
        # å°†å•ä¸ªArticleå¯¹è±¡è½¬æ¢ä¸ºå­—å…¸
        article_dict = {
            "title": article.title,
            "canonical_url": article.canonical_url,
            "description": article.description,
            "published_time": article.published_time,
            "subtype": article.subtype,
            "authors": [{"name": author.name, "topic_url": author.topic_url, "byline": author.byline} 
                       for author in article.authors] if article.authors else [],
            "thumbnail": {
                "caption": article.thumbnail.caption,
                "width": article.thumbnail.width,
                "height": article.thumbnail.height,
                "resizer_url": article.thumbnail.resizer_url
            } if article.thumbnail else None,
            "content_elements": article.content_elements[:10] if article.content_elements else []  # é™åˆ¶å†…å®¹å…ƒç´ æ•°é‡
        }
        
        return {
            "success": True,
            "article_detail": article_dict
        }
    except ApiError as e:
        return {
            "success": False,
            "error": f"Reuters APIé”™è¯¯: {str(e)}",
            "article_detail": None
        }
    except Exception as e:
        return {
            "success": False,
            "error": f"è·å–æ–‡ç« è¯¦æƒ…å¤±è´¥: {str(e)}",
            "article_detail": None
        }

# å·¥å…·å‡½æ•°ï¼šå¤šå…³é”®è¯æœç´¢ï¼ˆé«˜çº§æœç´¢ï¼‰
@mcp.tool()
def mcp_advanced_search(keywords: List[str], max_results_per_keyword: int = 10) -> dict:
    """å¤šå…³é”®è¯é«˜çº§æœç´¢
    
    å¯¹å¤šä¸ªå…³é”®è¯åˆ†åˆ«è¿›è¡Œæœç´¢å¹¶åˆå¹¶ç»“æœ
    
    Args:
        keywords: å…³é”®è¯åˆ—è¡¨ï¼Œå¦‚["technology", "AI", "blockchain"]
        max_results_per_keyword: æ¯ä¸ªå…³é”®è¯çš„æœ€å¤§ç»“æœæ•°ï¼Œé»˜è®¤10
        
    Returns:
        dict: åŒ…å«æ‰€æœ‰å…³é”®è¯æœç´¢ç»“æœçš„åˆå¹¶å­—å…¸
    """
    try:
        all_results = []
        search_summary = []
        
        # é™åˆ¶å…³é”®è¯æ•°é‡å’Œæ¯ä¸ªå…³é”®è¯çš„ç»“æœæ•°
        keywords = keywords[:5]  # æœ€å¤š5ä¸ªå…³é”®è¯
        max_results_per_keyword = min(max_results_per_keyword, 10)
        
        for keyword in keywords:
            try:
                search_results = reuters_client.search_articles(keyword=keyword, size=max_results_per_keyword)
                
                keyword_summary = {
                    "keyword": keyword,
                    "total_found": search_results.pagination.total_size,
                    "returned_count": len(search_results.articles) if search_results.articles else 0
                }
                search_summary.append(keyword_summary)
                
                if search_results.articles:
                    for article in search_results.articles:
                        article_dict = {
                            "search_keyword": keyword,  # æ ‡è®°æ¥æºå…³é”®è¯
                            "title": article.title,
                            "canonical_url": article.canonical_url,
                            "description": article.description,
                            "published_time": article.published_time,
                            "subtype": article.subtype,
                            "authors": [{"name": author.name, "topic_url": author.topic_url} 
                                       for author in article.authors] if article.authors else []
                        }
                        all_results.append(article_dict)
                        
            except Exception as e:
                keyword_summary = {
                    "keyword": keyword,
                    "error": str(e),
                    "total_found": 0,
                    "returned_count": 0
                }
                search_summary.append(keyword_summary)
        
        return {
            "success": True,
            "search_summary": search_summary,
            "total_articles_found": len(all_results),
            "combined_results": all_results
        }
        
    except Exception as e:
        return {
            "success": False,
            "error": f"é«˜çº§æœç´¢å¤±è´¥: {str(e)}",
            "combined_results": []
        }

# å·¥å…·å‡½æ•°ï¼šçƒ­é—¨ä¸»é¢˜æ–‡ç« èšåˆ
@mcp.tool()
def mcp_trending_topics(topics: List[str] = None, articles_per_topic: int = 5) -> dict:
    """è·å–çƒ­é—¨ä¸»é¢˜çš„æ–‡ç« èšåˆ
    
    Args:
        topics: ä¸»é¢˜åˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤çƒ­é—¨ä¸»é¢˜
        articles_per_topic: æ¯ä¸ªä¸»é¢˜çš„æ–‡ç« æ•°é‡ï¼Œé»˜è®¤5
        
    Returns:
        dict: åŒ…å«å„ä¸»é¢˜æ–‡ç« çš„èšåˆç»“æœ
    """
    try:
        # é»˜è®¤çƒ­é—¨ä¸»é¢˜
        if topics is None:
            topics = ["technology", "artificial intelligence", "cryptocurrency", "climate change", "electric vehicles"]
        
        # é™åˆ¶ä¸»é¢˜æ•°é‡å’Œæ¯ä¸ªä¸»é¢˜çš„æ–‡ç« æ•°
        topics = topics[:5]
        articles_per_topic = min(articles_per_topic, 10)
        
        trending_results = {}
        
        for topic in topics:
            try:
                # å…ˆå°è¯•ä½œä¸ºæœç´¢å…³é”®è¯
                search_results = reuters_client.search_articles(keyword=topic, size=articles_per_topic)
                
                topic_articles = []
                if search_results.articles:
                    for article in search_results.articles:
                        article_dict = {
                            "title": article.title,
                            "canonical_url": article.canonical_url,
                            "description": article.description[:200] + "..." if len(article.description) > 200 else article.description,
                            "published_time": article.published_time
                        }
                        topic_articles.append(article_dict)
                
                trending_results[topic] = {
                    "total_available": search_results.pagination.total_size,
                    "articles_returned": len(topic_articles),
                    "articles": topic_articles
                }
                
            except Exception as e:
                trending_results[topic] = {
                    "error": str(e),
                    "articles": []
                }
        
        return {
            "success": True,
            "trending_topics": trending_results
        }
        
    except Exception as e:
        return {
            "success": False,
            "error": f"è·å–çƒ­é—¨ä¸»é¢˜å¤±è´¥: {str(e)}",
            "trending_topics": {}
        }


@mcp.tool()
def cls_telegram():
    """ è·å–è´¢è”ç¤¾å®æ—¶ç”µæŠ¥ """
    # https://www.cls.cn/telegraph
    


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¯åŠ¨ Reuters MCP æœåŠ¡å™¨...")
    print("ğŸ“° å¯ç”¨åŠŸèƒ½:")
    print("  - mcp_search_articles: æœç´¢æ–‡ç« ")
    print("  - mcp_stock_articles: è·å–è‚¡ç¥¨ç›¸å…³æ–‡ç« ")
    print("  - mcp_topic_articles: è·å–ä¸»é¢˜æ–‡ç« ")
    print("  - mcp_section_articles: è·å–åˆ†ç±»æ–‡ç« ")
    print("  - mcp_article_by_url: è·å–æ–‡ç« è¯¦æƒ…")
    print("  - mcp_site_hierarchy: è·å–ç½‘ç«™ç»“æ„")
    print("  - mcp_advanced_search: é«˜çº§å¤šå…³é”®è¯æœç´¢")
    print("  - mcp_trending_topics: çƒ­é—¨ä¸»é¢˜èšåˆ")
    print("  - mcp_get_current_time: è·å–å½“å‰æ—¶é—´")
    
    try:
        mcp.run()
    except KeyboardInterrupt:
        print("\nğŸ‘‹ æœåŠ¡å™¨å·²åœæ­¢")
    except Exception as e:
        print(f"âŒ æœåŠ¡å™¨å¯åŠ¨å¤±è´¥: {e}")

if __name__ == "__main__":
    main()